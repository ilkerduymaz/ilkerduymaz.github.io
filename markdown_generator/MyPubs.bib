@article{duymazDistinctRulesBinding2023,
  title = {Distinct Rules for Binding in Position-Based and Velocity-Based Motion Systems},
  author = {Duymaz, Ilker and Alp, Nihan},
  date = {2023-08-01},
  journaltitle = {Journal of Vision},
  shortjournal = {Journal of Vision},
  volume = {23},
  number = {9},
  pages = {5248--5248},
  publisher = {{The Association for Research in Vision and Ophthalmology}},
  issn = {1534-7362},
  doi = {10.1167/jov.23.9.5248},
  url = {https://jov.arvojournals.org/article.aspx?articleid=2792133},
  urldate = {2025-11-19},
  langid = {english},
  file = {/home/ilkerduymaz/Zotero/storage/SILTZZH4/Duymaz and Alp - 2023 - Distinct rules for binding in position-based and velocity-based motion systems.pdf;/home/ilkerduymaz/Zotero/storage/94RNDBEE/article.html}
}

@article{duymazDistinctRulesPerceptual2025,
  title = {Distinct Rules for Perceptual Grouping in Position-Based and Velocity-Based Motion Systems},
  author = {Duymaz, Ilker and Alp, Nihan},
  date = {2025-11-01},
  journaltitle = {Attention, Perception, \& Psychophysics},
  shortjournal = {Atten Percept Psychophys},
  volume = {87},
  number = {8},
  pages = {2504--2512},
  issn = {1943-393X},
  doi = {10.3758/s13414-025-03135-1},
  url = {https://doi.org/10.3758/s13414-025-03135-1},
  urldate = {2025-11-17},
  abstract = {Motion perception relies on at least two distinct systems: a velocity-based motion system driven by early direction-selective cells and a position-based motion system that tracks objects over space and time. However, how these systems interact when operating in parallel remains unclear. We explored their respective contributions to the perceptual organization of motion using a bistable stimulus of eight moving dots, perceived either as rotating in local pairs (local motion percept) or as forming two illusory squares translating around fixation (global motion percept). To disrupt the velocity-based motion system, we varied interstimulus intervals (ISIs) stroboscopically from 0 to 116.6~ms – selectively impairing early direction-selective cells with short temporal integration windows (\$\${$<$}100\$\$~ms). Additionally, we manipulated contrast polarity to bias perceptual grouping (local-group, global-group, or no-group). We found that the pattern of perceptual bistability shifted markedly at ISIs of 33~ms. For ISIs \$\$\textbackslash ge \$\$33~ms, contrast- and proximity-based grouping strongly influenced perception. For ISIs \$\${$<$}33\$\$~ms, the global motion percept dominated even in the presence of strong static grouping cues (i.e., contrast and proximity), suggesting that the velocity-based motion system introduces a perceptual bias that can override or counteract static grouping cues. These findings reveal distinct, and at times opposing, contributions of velocity- and position-based motion systems to the perceptual organization of motion.},
  langid = {english},
  keywords = {Motion integration,Perceptual organization,Visual perception}
}

@online{duymazOriginNeuralFrequency2025,
  title = {Origin of {{Neural Frequency Responses}}: {{Intrinsic Sensory Coding}} vs. {{Structural Influences}}},
  shorttitle = {Origin of {{Neural Frequency Responses}}},
  author = {Duymaz, Ilker and Kogo, Naoki and Alp, Nihan},
  date = {2025-08-04},
  eprinttype = {bioRxiv},
  eprintclass = {New Results},
  pages = {2023.10.17.562345},
  issn = {2692-8205},
  doi = {10.1101/2023.10.17.562345},
  url = {https://www.biorxiv.org/content/10.1101/2023.10.17.562345v3},
  urldate = {2025-11-17},
  abstract = {Periodic changes in visual input can produce rhythmic patterns in EEG signals, which appear as narrowband frequency components. These components are commonly interpreted as reflecting the activity of neurons sensitive to the modulated stimulus features. Here, we present a scenario in which frequency components arise solely from retinotopic variations in signal strength, without reflecting any specific neural mechanism sensitive to the modulated feature. Using simulated and empirical data, we show that signal fluctuations based purely on retinotopic stimulus position can produce identifiable frequency components in response to position-modulated stimuli. These components likely reflect structural rather than functional cortical factors influencing signal strength across different retinotopic areas. Our results challenge the conventional assumption that frequency components necessarily indicate intrinsic neural signal processing, instead highlighting how interactions between stimuli and cortical architecture can give rise to such components.},
  langid = {english},
  pubstate = {prepublished},
  file = {/home/ilkerduymaz/Zotero/storage/YTSLS66B/Duymaz et al. - 2025 - Origin of Neural Frequency Responses Intrinsic Sensory Coding vs. Structural Influences.pdf}
}

@article{engeserCharacterizingInternalModels2025,
  title = {Characterizing Internal Models of the Visual Environment},
  author = {Engeser, Micha and Ajith, Susan and Duymaz, Ilker and Wang, Gongting and Foxwell, Matthew J. and Cichy, Radoslaw M. and Pitcher, David and Kaiser, Daniel},
  date = {2025-08},
  journaltitle = {Proceedings B},
  publisher = {The Royal Society},
  doi = {10.1098/rspb.2025.0602},
  url = {https://royalsocietypublishing.org/doi/10.1098/rspb.2025.0602},
  urldate = {2025-11-17},
  abstract = {Despite the complexity of real-world environments, natural vision is seamlessly efficient. To explain this efficiency, researchers often use predictive processing frameworks, in which perceptual efficiency is determined by the match between the visual ...},
  langid = {english},
  file = {/home/ilkerduymaz/Zotero/storage/GBU6Z3PN/rspb.2025.html}
}

@article{munnekeValuedrivenEffectsPerceptual2022,
  title = {Value-Driven Effects on Perceptual Averaging},
  author = {Munneke, Jaap and Duymaz, İlker and Corbett, Jennifer E.},
  date = {2022-02-09},
  journaltitle = {Attention, Perception, \& Psychophysics},
  shortjournal = {Atten Percept Psychophys},
  issn = {1943-393X},
  doi = {10.3758/s13414-022-02446-x},
  url = {https://doi.org/10.3758/s13414-022-02446-x},
  urldate = {2022-03-23},
  abstract = {Perceptual averaging refers to a strategy of encoding the statistical properties of entire sets of objects rather than encoding individual object properties, potentially circumventing the visual system’s strict capacity limitations. Prior work has shown that such average representations of set properties, such as its mean size, can be modulated by top-down and bottom-up attention. However, it is unclear to what extent attentional biases through selection history, in the form of value-driven attentional capture, influences this type of summary statistical representation. To investigate, we conducted two experiments in which participants estimated the mean size of a set of heterogeneously sized circles while a previously rewarded color singleton was part of the set. In Experiment 1, all circles were gray, except either the smallest or the largest circle, which was presented in a color previously associated with a reward. When the largest circle in the set was associated with the highest value (as a proxy of selection history), we observed the largest biases, such that perceived mean size scaled linearly with the increasing value of the attended color singleton. In Experiment 2, we introduced a dual-task component in the form of an attentional search task to ensure that the observed bias of reward on perceptual averaging was not fully explained by focusing attention solely on the reward-signaling color singleton. Collectively, findings support the proposal that selection history, like bottom-up and top-down attention, influences perceptual averaging, and that this happens in a flexible manner proportional to the extent to which attention is captured.},
  langid = {english},
  file = {/home/ilkerduymaz/Documents/Articles/Munneke et al_2022_Value-driven effects on perceptual averaging.pdf}
}

@article{senturkSabanciUniversityDynamic2022,
  title = {The {{Sabancı University Dynamic Face Database}} ({{SUDFace}}): {{Development}} and Validation of an Audiovisual Stimulus Set of Recited and Free Speeches with Neutral Facial Expressions},
  shorttitle = {The {{Sabancı University Dynamic Face Database}} ({{SUDFace}})},
  author = {Şentürk, Yağmur Damla and Tavacioglu, Ebru Ecem and Duymaz, İlker and Sayim, Bilge and Alp, Nihan},
  date = {2022-08-26},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  issn = {1554-3528},
  doi = {10.3758/s13428-022-01951-z},
  url = {https://doi.org/10.3758/s13428-022-01951-z},
  urldate = {2022-08-27},
  abstract = {Faces convey a wide range of information, including one’s identity, and emotional and mental states. Face perception is a major research topic in many research fields, such as cognitive science, social psychology, and neuroscience. Frequently, stimuli are selected from a range of available face databases. However, even though faces are highly dynamic, most databases consist of static face stimuli. Here, we introduce the Sabancı University Dynamic Face (SUDFace) database. The SUDFace database consists of 150 high-resolution audiovisual videos acquired in a controlled lab environment and stored with a resolution of 1920 × 1080 pixels at a frame rate of 60 Hz. The multimodal database consists of three videos of each human model in frontal view in three different conditions: vocalizing two scripted texts (conditions 1 and 2) and one Free Speech (condition 3). The main focus of the SUDFace database is to provide a large set of dynamic faces with neutral facial expressions and natural speech articulation. Variables such as face orientation, illumination, and accessories (piercings, earrings, facial hair, etc.) were kept constant across all stimuli. We provide detailed stimulus information, including facial features (pixel-wise calculations of face length, eye width, etc.) and speeches (e.g., duration of speech and repetitions). In two validation experiments, a total number of 227 participants rated each video on several psychological dimensions (e.g., neutralness and naturalness of expressions, valence, and the perceived mental states of the models) using Likert scales. The database is freely accessible for research purposes.},
  langid = {english},
  keywords = {Dynamic face,Face database,Face recognition,Natural face,Neutral face,Speech recognition},
  file = {/home/ilkerduymaz/Documents/Articles/Şentürk et al_2022_The Sabancı University Dynamic Face Database (SUDFace).pdf}
}

@online{wangHowVisualConceptual2025,
  title = {How Do Visual and Conceptual Factors Predict the Composition of Typical Scene Drawings?},
  author = {Wang, Gongting and Duymaz, Ilker and Foxwell, Matthew J. and Engeser, Micha and Pitcher, David and Cichy, Radoslaw M. and Kaiser, Daniel},
  date = {2025-09-21},
  eprinttype = {bioRxiv},
  eprintclass = {New Results},
  pages = {2025.09.15.676247},
  issn = {2692-8205},
  doi = {10.1101/2025.09.15.676247},
  url = {https://www.biorxiv.org/content/10.1101/2025.09.15.676247v2},
  urldate = {2025-11-19},
  abstract = {Imagine you are asked to draw a typical bedroom, what would you put on paper? Your choice of objects is likely to depend on visual occurrence statistics (i.e., the objects present in previously encountered bedrooms) and semantic relations between objects and scenes (i.e., the semantic relationship between the bedroom and its constituent objects). To investigate how these two factors contribute to the composition of typical scene drawings, we analyzed 1,192 drawings of six indoor scene categories, obtained from 303 participants. For each object featured in the drawings, we estimated its visual occurrence frequency from the ADE20K dataset of annotated scene images, and its semantic relatedness to the scene concept from a word2vec language processing model. Across all scenes of a given category, generalized linear models revealed that visual and conceptual factors both predicted the likelihood of an object featuring in the scene drawings, with a combined model outperforming both single-factor models. We further computed the visual and semantic specificity of objects for a given scene, that is, how diagnostic an object is for the scene. Object specificity offered only weak predictive power when predicting the selection of objects, yet even infrequently drawn objects remained diagnostic of their scenes. Taken together, we show that visual and conceptual factors jointly shape the composition of typical scene drawings. By releasing a large dataset of typical scene drawings alongside this work, we further provide a starting point for future studies exploring other critical properties of human drawings.},
  langid = {english},
  pubstate = {prepublished},
  file = {/home/ilkerduymaz/Zotero/storage/DCQHKB4S/Wang et al. - 2025 - How do visual and conceptual factors predict the composition of typical scene drawings.pdf}
}
